# Flink Everest Analytics Software: The pipeline analytics module for everest project

- [Flink Everest Analytics Software: The pipeline analytics module for everest project](#flink-everest-analytics-software-the-pipeline-analytics-module-for-everest-project)
  - [Info](#info)
  - [Audience](#audience)
  - [Requirements](#requirements)
  - [How to build](#how-to-build)
  - [How to run](#how-to-run)
  - [Everest Registry](#everest-registry)

## Info
This is a README for Everest Analytics Development (For Developer only) and it is subject to change.

## Audience
Developer with Java + Python (and Go if you plan to use Everest kafka go clients) package (TBD)

## Requirements
- JDK 8+ or higher
- Maven 3.5.4 or higher
- Public Internet access
- Everest project repo at https://github.com/iharijono/everest.git
- python 3.x
- go (optional, only if you want to use eliot go client)
- kivy (optional, only if you want to use python GUI to configure the registry, this can be done from command line too)
- python dependencies, install python dependencies from tools/requirements.txt
```
% pip install -r tools/requirements.txt
```
OR 

Install the dependencies one by one: 

- pytest, python coverage, python mock
```
% pip install pytest
```
```
% pip install pytest-cov
```
```
% pip install mock
```
- python tornado web
```
% pip install tornado
```
- python requests
```
% pip install requests
```
- python kafka client
```
% pip install kafka-python
```
- python cassandra client
```
% pip install cassandra-driver
```

## How to build
- clone the everest repo and go to the directory 'components/analytics/flink-everest'
- build with tests:  
`% mvn clean install`

- build without (Unit and Integration) tests (NOT RECOMMENDED):  
`% mvn clean install -DskipTests=true`

- build with go and its tests (WIP):  
`% mvn clean install -DskipGo=false`

- build the package as a docker image for kubernetes deployment (it will be stored at dockerhub:iharijono):  
  - go to directory tools:    
    `% cd tools`    
  - build the image:    
    `% ./build.sh`

## How to run
There are many ways to run/deploy the pipeline. Here are two ways to run it:  
- Local:  
    - Make sure you have kafka, cassandra, influxdb, grafana services running
- Kubernetes: WARNING: You will use the image that you built above!       
    - Make sure you have enough resources (nodes/memory/cpus) available in your kubernetes cluster  
        NOTES: Kafka, zookeeper (prometheus if you run it) and flink are resource intensive applications
    - Make sure you have kubernetes running and set your kube config
      correctly pointing to that cluster
    - Make sure you are at the directory 'everest/deployment/kubernetes/vm':  
        `% cd everest/deployment/kubernetes/vm` 
    - execute 'deploy.sh':  
        `% ./deploy.sh`
    - wait (can be 5-15 minutes)
    - on and off check if everything are started and running:  
        `% kubectl get pods -n everest`
    - if everything is running, you can inspect the eliot grafana dashboard or/and eliot flink dashboard, 
      you must know the IP address of one of the nodes (e.g. kube_node below) in your kubernetes cluster:       
        `% firefox http://<kube_node>:30003`         
        `% firefox http://<kube_node>:30081` 
    - (OPTIONAL) If you care about sensor registration, register the sensors (you can do this anytime), see 'Sensor Registration' below.  
    - With flink web ui (http://<kube_node>:30081), submit the job
    - With grafana ui (http://<kube_node>:30003), observe the movement of the data/events
    - Now you are ready to receive data from flink eliot job: TBD TBD TBD TBD
        - go to directory 'tools/sensors/sim/src/receiver/kafka' and start kafka consumer, the incoming data here are generated by eliot flink job:    
        `% python consumer.py -i h-topic -k <kube_node>:32400`  
        `% python consumer.py -i l-topic -k <kube_node>:32400`  
        `% python consumer.py -i c-topic -k <kube_node>:32400`  
            
          if you need help or see how to use the program, invoke the python program with '-h':  
        `% python consumer.py -h`           
        - go to directory 'tools/sensors/sim/src' and simulate/send data to Kafka which will be processed by eliot flink job:     
        `% python sim.py -k <kube_node>:32400 -g 3 -s 10 -d 3m`  
           This will simulate 30 sensors coming from 3 Gateway (10 sensors each).  
           The ID of the sensors are in the form GatewayID:SensorID, for example 'ST0:SID0', 'ST0:SID1' etc...  
            
          if you need help or see how to use the program, invoke the python program with '-h':  
        `% python sim.py -h`
    - Observe grafana ui for eliot job and its output data at http://<kube_node>:30003         
        

## Everest Registry
If you want to manage the rules/actions, use everest registry.  
- Using command line:  
    - Enter (login) into the container 'everest-tools'  
        `% kubectl exec -it eliot-tools -- bash` 
    - Inside the container, go to the directory /app/everest/sim/src  
        `% cd /app/everest/sim/src`   
    - Configure the rules and actions:  
        `% python registry_app.py TBD`   
      Note: TBD  
            TBD etc...  
    - To see what rules and actions are registered  
        `% python registry_app.py -l` 
        
- Using GUI:  
    - Make sure you have python kivy package installed on your computer  
        `% pip install kivy` 
    - TBD ...   

